{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a76493d",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8c2c2",
   "metadata": {},
   "source": [
    "## Fine-tuned Sentiment Analysis Model with Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3587179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset, ClassLabel, load_from_disk, DatasetDict, load_metric\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import pipeline\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29589031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset emotion (/root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    }
   ],
   "source": [
    "# read the full data\n",
    "dataset = load_dataset(\"emotion\", \"default\", split = 'train')\n",
    "full_df = pd.DataFrame(dataset, columns = ['text', 'label'])\n",
    "full_df = full_df[0:10000]\n",
    "# full_df = pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e7e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'text'\n",
    "label_col = 'label'\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde4b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faa694ce3fa4972a60f32f97666dd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b075b2e831b4a6e8162754fe1125365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af42c1a43c6746b59aca0327bc7f5109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alldata_ds = Dataset.from_pandas(full_df)\n",
    "alldata_ds = alldata_ds.class_encode_column(label_col)\n",
    "data_ds = alldata_ds.train_test_split(test_size=0.4, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72c3d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03349fe176543d685ee8af886ed5808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12901788ebba4acb9d35c75de4521fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define tokenizing function\n",
    "def tokenize_inputs(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    return tokenizer(text[text_col], truncation=True)\n",
    "    \n",
    "# do the tokenizing using map function\n",
    "tokenized_ds = data_ds.map(tokenize_inputs, batched=True,\n",
    "                           remove_columns = list(set(full_df.columns.to_list()).difference(set([text_col, label_col]))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50a3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = data_ds['train'].features[label_col].num_classes\n",
    "id2label = {ind:label for ind, label in enumerate(data_ds['train'].features[label_col].names)}\n",
    "label2id = {label:ind for ind, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8980ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels = no_classes,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da2eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_arg = TrainingArguments(\"test-trainer\",\n",
    "                                 logging_strategy='epoch',\n",
    "                                 evaluation_strategy = 'epoch',\n",
    "                                 save_strategy = 'epoch', \n",
    "                                 load_best_model_at_end = True,\n",
    "                                 metric_for_best_model='fscore',\n",
    "                                 greater_is_better=True,\n",
    "                                 report_to = 'all',\n",
    "                                 per_device_train_batch_size = 8,\n",
    "                                 per_device_eval_batch_size = 8, \n",
    "                                 num_train_epochs = 3,\n",
    "                                 seed = 42\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eee8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    #get predictions by using index of max logit\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    #calculate classification report\n",
    "    perfs = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
    "    perf_dict = dict(zip(['precision', 'recall', 'fscore'], perfs[:3]))\n",
    "    \n",
    "    #return dictionary\n",
    "    return perf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b396fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model = model,\n",
    "                  args=training_arg,\n",
    "                  data_collator = data_collator,\n",
    "                  tokenizer=tokenizer,\n",
    "                  train_dataset = tokenized_ds['train'],\n",
    "                  eval_dataset = tokenized_ds['test'],\n",
    "                  compute_metrics = compute_metrics\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d28635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 282\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 01:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.293881</td>\n",
       "      <td>0.893833</td>\n",
       "      <td>0.869976</td>\n",
       "      <td>0.880484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.317740</td>\n",
       "      <td>0.898773</td>\n",
       "      <td>0.878058</td>\n",
       "      <td>0.886347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.291555</td>\n",
       "      <td>0.888525</td>\n",
       "      <td>0.892350</td>\n",
       "      <td>0.890235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-trainer/checkpoint-94\n",
      "Configuration saved in test-trainer/checkpoint-94/config.json\n",
      "Model weights saved in test-trainer/checkpoint-94/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-94/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-94/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-trainer/checkpoint-188\n",
      "Configuration saved in test-trainer/checkpoint-188/config.json\n",
      "Model weights saved in test-trainer/checkpoint-188/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-188/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-188/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-trainer/checkpoint-282\n",
      "Configuration saved in test-trainer/checkpoint-282/config.json\n",
      "Model weights saved in test-trainer/checkpoint-282/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-282/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-282/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from test-trainer/checkpoint-282 (score: 0.8902352553671221).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=0.07175415035680677, metrics={'train_runtime': 90.901, 'train_samples_per_second': 198.018, 'train_steps_per_second': 3.102, 'total_flos': 525204972676224.0, 'train_loss': 0.07175415035680677, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44671d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29155483841896057, 'eval_precision': 0.8885248590841356, 'eval_recall': 0.8923499028752908, 'eval_fscore': 0.8902352553671221, 'eval_runtime': 6.5057, 'eval_samples_per_second': 614.847, 'eval_steps_per_second': 9.684, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_ds = trainer.evaluate(tokenized_ds['test'])\n",
    "print(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7657de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183271ab",
   "metadata": {},
   "source": [
    "## Using Specified Pipeline without Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6149b89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 00:31:14.453507 140547989231424 builder.py:412] Using custom data configuration default\n",
      "W0907 00:31:14.456440 140547989231424 builder.py:577] Reusing dataset emotion (/root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       100 non-null    object\n",
      " 1   label      100 non-null    int64 \n",
      " 2   label_str  100 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"emotion\", \"default\", split = 'validation')\n",
    "full_df = pd.DataFrame(dataset, columns = ['text', 'label'])\n",
    "full_df = full_df[0:100]\n",
    "full_df.loc[full_df['label'] == 0, 'label_str'] = 'sadness'\n",
    "full_df.loc[full_df['label'] == 1, 'label_str'] = 'joy'\n",
    "full_df.loc[full_df['label'] == 2, 'label_str'] = 'love'\n",
    "full_df.loc[full_df['label'] == 3, 'label_str'] = 'anger'\n",
    "full_df.loc[full_df['label'] == 4, 'label_str'] = 'fear'\n",
    "full_df.loc[full_df['label'] == 5, 'label_str'] = 'surprise'\n",
    "full_df.info()\n",
    "# full_df = pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40bfb013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sadness    0.39\n",
       "joy        0.28\n",
       "anger      0.16\n",
       "love       0.13\n",
       "fear       0.04\n",
       "Name: label_str, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['label_str'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34185c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_analysis(df, mypipeline:str, mylabels:list, text_col:str):\n",
    "    \"\"\"\n",
    "    run sentiment analysis on text using specified pipeline\n",
    "    \n",
    "    Args:\n",
    "        df: data frame\n",
    "        mypipeline (str): specified pipeline \n",
    "        mylabes (list): list of specified labels\n",
    "        text_col (str): name of text column\n",
    "        \n",
    "    Returns:\n",
    "        predictions (list): predicted labels with the highest scores\n",
    "        scores (list): sentiment analysis scores for the predictions\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    classifier = pipeline(mypipeline, device = 0)\n",
    "    candidate_labels = mylabels\n",
    "    preds = [classifier(sequence, candidate_labels) for sequence in df[text_col].tolist()]\n",
    "    predictions = [pred['labels'][0] for pred in preds]\n",
    "    scores = [pred['scores'][0] for pred in preds]\n",
    "    print(time.time() - start_time, 'seconds')\n",
    "    return predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6149ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n",
      "loading configuration file https://huggingface.co/facebook/bart-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/980f2be6bd282c5079e99199d7554cfd13000433ed0fdc527e7def799e5738fe.4fdc7ce6768977d347b32986aff152e26fcebbda34ef89ac9b114971d0342e09\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-mnli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/facebook/bart-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/980f2be6bd282c5079e99199d7554cfd13000433ed0fdc527e7def799e5738fe.4fdc7ce6768977d347b32986aff152e26fcebbda34ef89ac9b114971d0342e09\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-mnli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/bart-large-mnli/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/35014754ae1fcb956d44903df02e4f69d0917cab0901ace5ac7f4a4a998346fe.a30bb5d685bb3c6e9376ab4480f1b252d9796d438d1c84a9b2deb0275c5b2151\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at facebook/bart-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "loading configuration file https://huggingface.co/facebook/bart-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/980f2be6bd282c5079e99199d7554cfd13000433ed0fdc527e7def799e5738fe.4fdc7ce6768977d347b32986aff152e26fcebbda34ef89ac9b114971d0342e09\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-mnli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/facebook/bart-large-mnli/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/b4f8395edd321fd7cd8a87bca767b1135680a41d8931516dd1a447294633b9db.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "loading file https://huggingface.co/facebook/bart-large-mnli/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/19c09c9654551e163f858f3c99c226a8d0026acc4935528df3b09179204efe4c.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/facebook/bart-large-mnli/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/540455855ce0a3c13893c5d090d142de9481365bd32dc5457c957e5d13444d23.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/facebook/bart-large-mnli/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-large-mnli/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-large-mnli/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/569800088d6f014777e6d5d8cb61b2b8bb3d18a508a1d8af041aae6bbc6f3dfe.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
      "loading configuration file https://huggingface.co/facebook/bart-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/980f2be6bd282c5079e99199d7554cfd13000433ed0fdc527e7def799e5738fe.4fdc7ce6768977d347b32986aff152e26fcebbda34ef89ac9b114971d0342e09\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-mnli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.893474340438843 seconds\n"
     ]
    }
   ],
   "source": [
    "predictions, scores = run_sentiment_analysis(df = full_df, \n",
    "                                             mypipeline = \"zero-shot-classification\", \n",
    "                                             mylabels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"], \n",
    "                                             text_col = 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fcda7d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.90      0.56      0.69        16\n",
      "        fear       0.00      0.00      0.00         4\n",
      "         joy       0.89      0.29      0.43        28\n",
      "        love       0.50      0.31      0.38        13\n",
      "     sadness       0.82      0.46      0.59        39\n",
      "    surprise       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.39       100\n",
      "   macro avg       0.52      0.27      0.35       100\n",
      "weighted avg       0.78      0.39      0.51       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(full_df['label_str'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e865b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.942836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.771841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>0.752808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.552044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.549333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i start to feel more appreciative of what god ...</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.435614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i am feeling more confident that we will be ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.328114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i feel incredibly lucky just to be able to tal...</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.391188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i feel less keen about the army every day</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.376403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i feel dirty and ashamed for saying that</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.336696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_str  \\\n",
       "0  im feeling quite sad and sorry for myself but ...      0   sadness   \n",
       "1  i feel like i am still looking at a blank canv...      0   sadness   \n",
       "2                     i feel like a faithful servant      2      love   \n",
       "3                  i am just feeling cranky and blue      3     anger   \n",
       "4  i can have for a treat or if i am feeling festive      1       joy   \n",
       "5  i start to feel more appreciative of what god ...      1       joy   \n",
       "6  i am feeling more confident that we will be ab...      1       joy   \n",
       "7  i feel incredibly lucky just to be able to tal...      1       joy   \n",
       "8          i feel less keen about the army every day      1       joy   \n",
       "9           i feel dirty and ashamed for saying that      0   sadness   \n",
       "\n",
       "  sentiment     score  \n",
       "0   sadness  0.942836  \n",
       "1  surprise  0.771841  \n",
       "2      love  0.752808  \n",
       "3   sadness  0.552044  \n",
       "4       joy  0.549333  \n",
       "5  surprise  0.435614  \n",
       "6       joy  0.328114  \n",
       "7  surprise  0.391188  \n",
       "8  surprise  0.376403  \n",
       "9  surprise  0.336696  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = full_df.copy()\n",
    "df['sentiment'] = predictions\n",
    "df['score'] = scores\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f9a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
